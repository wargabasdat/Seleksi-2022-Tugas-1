{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotten Tomatoes Web Scraper\n",
    "### Gresya Angelina Eunike Leman (18220104)\n",
    "### Tugas Seleksi Calon Asisten Lab Basis Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables, Functions, and Procedures Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.rottentomatoes.com\"\n",
    "series = []\n",
    "\n",
    "# rest after scraping each page's data\n",
    "def rest(count):\n",
    "    if(count % 20 == 0):\n",
    "        print(f\"Done scraping the contents of {count} tv shows.\")\n",
    "        time.sleep(3)\n",
    "    time.sleep(2)\n",
    "\n",
    "# format date to ISO 8601\n",
    "def dateformatter(date):\n",
    "    months = {\n",
    "        'Jan' : '01',\n",
    "        'Feb' : '02',\n",
    "        'Mar' : '03',\n",
    "        'Apr' : '04',\n",
    "        'May' : '05',\n",
    "        'Jun' : '06',\n",
    "        'Jul' : '07',\n",
    "        'Aug' : '08',\n",
    "        'Sep' : '09',\n",
    "        'Oct' : '10',\n",
    "        'Nov' : '11',\n",
    "        'Dec' : '12',\n",
    "    }\n",
    "    date = date.replace(',', '').split(' ')\n",
    "    date[0] = months.get(date[0])\n",
    "    date[0], date[1], date[2] = date[2], date[0], date[1]\n",
    "    date = '-'.join(date)\n",
    "    return (date)\n",
    "\n",
    "# format text\n",
    "def formattext(string):\n",
    "    return (string.text.strip())\n",
    "\n",
    "# format rating to integer\n",
    "def ratingformatter(rating):\n",
    "    return (int(formattext(rating).replace('%', '')))\n",
    "\n",
    "# remove parantheses\n",
    "def removepar(string):\n",
    "    return (string.replace('(', '').replace(')', ''))\n",
    "\n",
    "# procedure to write the collected data to a json file\n",
    "def write_json(datas):\n",
    "    path = 'D:\\seleksi asisten basdat\\Seleksi-2022-Tugas-1\\Data Scraping\\data'\n",
    "    file_name = 'TvShows.json'    # name of the file\n",
    "    file_path = os.path.join(path, file_name)\n",
    "    with open(file_path, 'w', encoding = 'utf-8') as file:\n",
    "        json.dump(datas, file, ensure_ascii = False, indent = 4)\n",
    "    print(\"Done exporting json file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing HTML of the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the browser window\n",
    "path = 'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chromedriver.exe'\n",
    "service = Service(path)\n",
    "service.start()\n",
    "driver = webdriver.Remote(service.service_url)\n",
    "\n",
    "# navigates driver to the url containing the first page \n",
    "url = url = baseurl + '/browse/tv_series_browse/sort:popular?page=1'\n",
    "driver.get(url)\n",
    "\n",
    "# create load more button object\n",
    "loadmore = driver.find_element(By.XPATH, '/html/body/div[3]/main/div[1]/div/div[5]/button')\n",
    "\n",
    "# clicking the button as many times to reach go to last desired page\n",
    "lastpage = 50\n",
    "for i in range(lastpage-1):\n",
    "    loadmore.click()\n",
    "    time.sleep(4)   # resting after each clicks\n",
    "\n",
    "# grab the html of the page\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done scraping the contents of 20 tv shows.\n",
      "Done scraping the contents of 40 tv shows.\n",
      "Done scraping the contents of 60 tv shows.\n",
      "Done scraping the contents of 80 tv shows.\n",
      "Done scraping the contents of 100 tv shows.\n",
      "Done scraping the contents of 120 tv shows.\n",
      "Done scraping the contents of 140 tv shows.\n",
      "Done scraping the contents of 160 tv shows.\n",
      "Done scraping the contents of 180 tv shows.\n",
      "Done scraping the contents of 200 tv shows.\n",
      "Done scraping the contents of 220 tv shows.\n",
      "Done scraping the contents of 240 tv shows.\n",
      "Done scraping the contents of 260 tv shows.\n",
      "Done scraping the contents of 280 tv shows.\n",
      "Done scraping the contents of 300 tv shows.\n",
      "Done scraping the contents of 320 tv shows.\n",
      "Done scraping the contents of 340 tv shows.\n",
      "Done scraping the contents of 360 tv shows.\n",
      "Done scraping the contents of 380 tv shows.\n",
      "Done scraping the contents of 400 tv shows.\n",
      "Done scraping the contents of 420 tv shows.\n",
      "Done scraping the contents of 440 tv shows.\n",
      "Done scraping the contents of 460 tv shows.\n",
      "Done scraping the contents of 480 tv shows.\n",
      "Done scraping the contents of 500 tv shows.\n",
      "Done scraping the contents of 520 tv shows.\n",
      "Done scraping the contents of 540 tv shows.\n",
      "Done scraping the contents of 560 tv shows.\n",
      "Done scraping the contents of 580 tv shows.\n",
      "Done scraping the contents of 600 tv shows.\n",
      "Done scraping the contents of 620 tv shows.\n",
      "Done scraping the contents of 640 tv shows.\n",
      "Done scraping the contents of 660 tv shows.\n",
      "Done scraping the contents of 680 tv shows.\n",
      "Done scraping the contents of 700 tv shows.\n",
      "Done scraping the contents of 720 tv shows.\n",
      "Done scraping the contents of 740 tv shows.\n",
      "Done scraping the contents of 760 tv shows.\n",
      "Done scraping the contents of 780 tv shows.\n",
      "Done scraping the contents of 800 tv shows.\n",
      "Done scraping the contents of 820 tv shows.\n",
      "Done scraping the contents of 840 tv shows.\n",
      "Done scraping the contents of 860 tv shows.\n",
      "Successfully scraped the contents of 876 tv shows.\n"
     ]
    }
   ],
   "source": [
    "# making a beautiful soup to grab the datas\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# finding all of the links to the tv shows\n",
    "tvshows = soup.find('div', class_ = 'discovery-grids-container').find_all('a')\n",
    "\n",
    "# declaration of variabled used\n",
    "headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.74 Safari/537.36'}\n",
    "count = 1\n",
    "\n",
    "for link in tvshows:\n",
    "    try:\n",
    "        # declaring the next url, extracting the html, and turning it into a soup\n",
    "        suburl = baseurl + link['href']\n",
    "        subsource = requests.get(suburl, headers=headers).text\n",
    "        subsoup = BeautifulSoup(subsource, 'lxml')\n",
    "\n",
    "        # fetching title, airing years, and the synopsis of the tv show\n",
    "        title = formattext(subsoup.find('h1', class_ = 'mop-ratings-wrap__title mop-ratings-wrap__title--top'))\n",
    "        airing = removepar(formattext(subsoup.find('span', class_ = 'h3 subtle')))\n",
    "        synopsis = formattext(subsoup.find('div', class_ = 'tv-series__series-info--synopsis clamp clamp-6 js-clamp clearfix'))\n",
    "\n",
    "        # fetching the average rating of each seasons (tomatometer and audience score)\n",
    "        avgtm = ratingformatter(subsoup.find('span', {'data-qa' : 'tomatometer'}))\n",
    "        avgas = ratingformatter(subsoup.find('span', {'data-qa' : 'audience-score'}))\n",
    "\n",
    "        # fetching the tv network, premiere date, and genre\n",
    "        tvnetwork = formattext(subsoup.find('td', {'data-qa' : 'series-details-network'}))        \n",
    "        pdate = dateformatter(formattext(subsoup.find('td', {'data-qa' : 'series-details-premiere-date'}))) \n",
    "        genre = formattext(subsoup.find('td', {'data-qa' : 'series-details-genre'}))\n",
    "\n",
    "        # fetching main casts of each series\n",
    "        maincasts = [] \n",
    "        casts = subsoup.find('span', {'data-qa' : 'series-info-cast'}).find_next_siblings(\"a\")\n",
    "        for cast in casts:\n",
    "            maincasts.append(formattext(cast))\n",
    "\n",
    "        # fetching data from every season of the series\n",
    "        seasons = subsoup.find_all('a', {'data-qa' : 'season-link'})\n",
    "        seasoninfo = []\n",
    "        for season in seasons:\n",
    "            try:\n",
    "                # fetching details provided before the shadow dom\n",
    "                sinfo = season.find('season-list-item')\n",
    "                info = sinfo['info'].split(\",\")         # var to keep number of episodes and airing year\n",
    "\n",
    "                # fetching the title, number of episodes, and airing year of each season\n",
    "                stitle = title + ': ' + sinfo['seasontitle']\n",
    "                episodes = int(info[2].strip().replace(' episodes', ''))\n",
    "                airing_year = int(info[0].strip())\n",
    "\n",
    "                # declaring every season's url and extracting the html, also turning it into a soup\n",
    "                surl = baseurl + season['href']\n",
    "                ssource = requests.get(surl, headers=headers).text\n",
    "                ssoup = BeautifulSoup(ssource, 'lxml')\n",
    "\n",
    "                # fetching rating (tomatometer and audience score) form internal link\n",
    "                tomatometer = ratingformatter(ssoup.find('span', {'data-qa' : 'tomatometer'}))\n",
    "                audiencescore = ratingformatter(ssoup.find('span', {'data-qa' : 'audience-score'}))\n",
    "\n",
    "                # keeping the season info\n",
    "                new_season = {\n",
    "                    \"season_title\" : stitle,\n",
    "                    \"airing_year\" : airing_year,\n",
    "                    \"episodes\" : episodes,\n",
    "                    \"tomatometer\" : tomatometer,\n",
    "                    \"audience_score\" : audiencescore,\n",
    "                }\n",
    "\n",
    "                # reversing the array and adding the season info to an array\n",
    "                seasoninfo.reverse()\n",
    "                seasoninfo.append(new_season)\n",
    "\n",
    "                # resting time before looping (so it's not blocked)\n",
    "                rest(1)\n",
    "\n",
    "            # passing the bad datas\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        # keeping each series info\n",
    "        new_series = {\n",
    "            \"title\" : title,\n",
    "            \"airing\" : airing,\n",
    "            \"synopsis\" : synopsis,\n",
    "            \"average_tomatometer\" : avgtm,\n",
    "            \"average_audience_score\" : avgas,\n",
    "            \"tv_network\" : tvnetwork,\n",
    "            \"premiere_date\" : pdate,\n",
    "            \"genre\" : genre,\n",
    "            \"main_casts\" : maincasts,\n",
    "            \"num_of_seasons\" : len(seasoninfo),\n",
    "            \"seasons_info\" : seasoninfo,\n",
    "        }\n",
    "\n",
    "        #adding new series info to the list\n",
    "        series.append(new_series)\n",
    "        #resting time before looping\n",
    "        rest(count)\n",
    "        count += 1      # increasing the count of series\n",
    "        \n",
    "    # passing the bad datas\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "print(f\"Successfully scraped the contents of {count - 1} tv shows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done exporting json file.\n",
      "Web scraping is finished!\n"
     ]
    }
   ],
   "source": [
    "write_json(series)\n",
    "print(\"Web scraping is finished!\")\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "668966952cf4f56b5597e225430528eb36a4751e64ba06075f6712eb54326feb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
